#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø³ÛŒØ³ØªÙ… Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ø§ Ø§Ù†ØªÙ‚Ø§Ù„ Ø´Ø¨Ø§Ù†Ù‡ Ø¨Ù‡ MongoDB
"""

import os
import json
import sqlite3
from datetime import datetime, time, timedelta
import schedule
import threading
import logging
import time as time_module
from typing import Dict, List, Any
import numpy as np
from openai import OpenAI

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ÛŒØ³ØªÙ… Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡
try:
    from monitoring.telemetry_logger import telemetry, log_function_performance, AlertLevel, MetricType
    from monitoring.alert_system import alert_system, AlertRule, AlertType, NotificationChannel
    MONITORING_ENABLED = True
except ImportError:
    MONITORING_ENABLED = False
    def log_function_performance(func):
        return func

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DailyDataCollectionSystem:
    def __init__(self):
        self.temp_db = 'temp_daily_data.db'
        self.analysis_db = 'daily_analysis.db'
        self.mongodb_queue = 'mongodb_queue.json'
        
        # Ø³Ø§Ø¹Øª Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ MongoDB (23:30)
        self.transfer_time = time(23, 30)
        
        # Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ùˆ ÙˆØ²Ù†â€ŒÙ‡Ø§
        self.scoring_weights = {
            'price_movement': 0.25,
            'volume': 0.20,
            'news_sentiment': 0.20,
            'technical_indicators': 0.15,
            'ai_prediction': 0.20
        }
        
        # Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ÛŒ
        self.data_collection_count = 0
        self.analysis_count = 0
        self.mongodb_transfers = 0
        self.last_collection_errors = []
        self.database_sizes = {}
        
        # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        self._initialize_databases()
        self.openai_client = OpenAI() if os.environ.get('OPENAI_API_KEY') else None
        
        # ØªÙ†Ø¸ÛŒÙ… Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯
        if MONITORING_ENABLED:
            self._setup_monitoring()
    
    def _setup_monitoring(self):
        """ØªÙ†Ø¸ÛŒÙ… Ø³ÛŒØ³ØªÙ… Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        try:
            # Ù‚Ø§Ù†ÙˆÙ† Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ø§Ù„Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            db_size_rule = AlertRule(
                id="database_size_warning",
                name="Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ø§Ù„Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³",
                alert_type=AlertType.CUSTOM,
                condition="temp_db_size_mb > 100",  # Ø¨ÛŒØ´ Ø§Ø² 100 Ù…Ú¯Ø§Ø¨Ø§ÛŒØª
                severity=AlertLevel.WARNING,
                channels=[NotificationChannel.LOG],
                cooldown_minutes=60
            )
            alert_system.add_rule(db_size_rule)
            
            # Ù‚Ø§Ù†ÙˆÙ† Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ
            collection_error_rule = AlertRule(
                id="data_collection_errors",
                name="Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡",
                alert_type=AlertType.CUSTOM,
                condition="collection_error_rate > 0.1",  # Ø¨ÛŒØ´ Ø§Ø² 10% Ø®Ø·Ø§
                severity=AlertLevel.ERROR,
                channels=[NotificationChannel.LOG],
                cooldown_minutes=30
            )
            alert_system.add_rule(collection_error_rule)
            
            # Ù‚Ø§Ù†ÙˆÙ† Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¹Ø¯Ù… Ø§Ù†ØªÙ‚Ø§Ù„ MongoDB
            mongodb_transfer_rule = AlertRule(
                id="mongodb_transfer_failed",
                name="Ù…Ø´Ú©Ù„ Ø¯Ø± Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ MongoDB",
                alert_type=AlertType.CUSTOM,
                condition="hours_since_last_transfer > 25",  # Ø¨ÛŒØ´ Ø§Ø² 25 Ø³Ø§Ø¹Øª
                severity=AlertLevel.CRITICAL,
                channels=[NotificationChannel.LOG],
                cooldown_minutes=120
            )
            alert_system.add_rule(mongodb_transfer_rule)
            
            logger.info("ğŸ”§ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ÙØ¹Ø§Ù„ Ø´Ø¯")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙ†Ø¸ÛŒÙ… Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯: {e}")
    
    def _initialize_databases(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯Ø§ÙˆÙ„ Ù…ÙˆÙ‚Øª Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡"""
        # Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆÙ‚Øª Ø±ÙˆØ²Ø§Ù†Ù‡
        conn = sqlite3.connect(self.temp_db)
        cursor = conn.cursor()
        
        # Ø¬Ø¯ÙˆÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS raw_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                data_type TEXT,
                market TEXT,
                symbol TEXT,
                data JSON,
                processed BOOLEAN DEFAULT 0
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analysis_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                symbol TEXT,
                analysis_type TEXT,
                score REAL,
                details JSON
            )
        ''')
        
        conn.commit()
        conn.close()
        
        # Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        conn = sqlite3.connect(self.analysis_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS daily_summaries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT,
                symbol TEXT,
                final_score REAL,
                recommendation TEXT,
                analysis JSON,
                transferred_to_mongodb BOOLEAN DEFAULT 0
            )
        ''')
        
        conn.commit()
        conn.close()
    
    @log_function_performance
    def collect_market_data(self, symbol: str, market: str) -> Dict:
        """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±"""
        start_time = time_module.time()
        
        try:
            data = {
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,
                'market': market,
                'price': np.random.uniform(100, 50000),  # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù‚ÛŒÙ…Øª
                'volume': np.random.uniform(1000, 100000),
                'change_24h': np.random.uniform(-10, 10),
                'high_24h': np.random.uniform(100, 51000),
                'low_24h': np.random.uniform(99, 49000)
            }
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆÙ‚Øª
            conn = sqlite3.connect(self.temp_db)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO raw_data (timestamp, data_type, market, symbol, data)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                data['timestamp'],
                'market_data',
                market,
                symbol,
                json.dumps(data)
            ))
            
            conn.commit()
            conn.close()
            
            # Ø´Ù…Ø§Ø±Ø´ Ù…ÙˆÙÙ‚
            self.data_collection_count += 1
            collection_time = time_module.time() - start_time
            
            # Ø«Ø¨Øª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
            if MONITORING_ENABLED:
                telemetry.record_metric("data_collection.count", self.data_collection_count, MetricType.COUNTER)
                telemetry.record_metric("data_collection.time", collection_time, MetricType.TIMER)
                telemetry.record_metric(f"market_data.{market}.{symbol}.price", data['price'], MetricType.GAUGE)
                telemetry.record_metric(f"market_data.{market}.{symbol}.volume", data['volume'], MetricType.GAUGE)
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                self._check_database_sizes()
            
            logger.info(f"ğŸ“Š Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯: {symbol} - Ù‚ÛŒÙ…Øª: ${data['price']:.2f}")
            return data
            
        except Exception as e:
            # Ø«Ø¨Øª Ø®Ø·Ø§
            self.last_collection_errors.append({
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,
                'market': market,
                'error': str(e)
            })
            
            # Ø­ÙØ¸ Ø­Ø¯Ø§Ú©Ø«Ø± 100 Ø®Ø·Ø§ÛŒ Ø§Ø®ÛŒØ±
            if len(self.last_collection_errors) > 100:
                self.last_collection_errors.pop(0)
            
            if MONITORING_ENABLED:
                telemetry.record_error(
                    AlertLevel.ERROR,
                    f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ {symbol}: {str(e)}",
                    "DataCollection",
                    str(e)
                )
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø±Ø® Ø®Ø·Ø§
                error_rate = len(self.last_collection_errors) / max(self.data_collection_count, 1)
                telemetry.record_metric("data_collection.error_rate", error_rate, MetricType.GAUGE)
            
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ {symbol}: {e}")
            raise
    
    def _check_database_sizes(self):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§"""
        try:
            temp_db_size = os.path.getsize(self.temp_db) if os.path.exists(self.temp_db) else 0
            analysis_db_size = os.path.getsize(self.analysis_db) if os.path.exists(self.analysis_db) else 0
            
            self.database_sizes = {
                'temp_db_size_bytes': temp_db_size,
                'temp_db_size_mb': temp_db_size / (1024 * 1024),
                'analysis_db_size_bytes': analysis_db_size,
                'analysis_db_size_mb': analysis_db_size / (1024 * 1024)
            }
            
            # Ø«Ø¨Øª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
            telemetry.record_metric("database.temp_size_mb", self.database_sizes['temp_db_size_mb'], MetricType.GAUGE)
            telemetry.record_metric("database.analysis_size_mb", self.database_sizes['analysis_db_size_mb'], MetricType.GAUGE)
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø´Ø¯Ø§Ø± Ø§Ù†Ø¯Ø§Ø²Ù‡
            if self.database_sizes['temp_db_size_mb'] > 100:
                telemetry.record_error(
                    AlertLevel.WARNING,
                    f"Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆÙ‚Øª Ø¨Ø§Ù„Ø§ Ø§Ø³Øª: {self.database_sizes['temp_db_size_mb']:.1f} MB",
                    "DataCollection"
                )
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
    
    @log_function_performance
    def analyze_and_score(self, symbol: str) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ùˆ Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        try:
            conn = sqlite3.connect(self.temp_db)
            cursor = conn.cursor()
            
            # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ù…Ø±ÙˆØ²
            cursor.execute('''
                SELECT data FROM raw_data 
                WHERE symbol = ? AND DATE(timestamp) = DATE('now')
                AND processed = 0
            ''', (symbol,))
            
            raw_data = cursor.fetchall()
            
            if not raw_data:
                return {}
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª
            scores = {
                'price_movement': self._calculate_price_score(raw_data),
                'volume': self._calculate_volume_score(raw_data),
                'news_sentiment': self._calculate_news_sentiment(symbol),
                'technical_indicators': self._calculate_technical_score(raw_data),
                'ai_prediction': self._get_ai_prediction_score(symbol, raw_data)
            }
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ
            final_score = sum(
                scores[key] * self.scoring_weights[key] 
                for key in scores
            )
            
            # ØªØ¹ÛŒÛŒÙ† ØªÙˆØµÛŒÙ‡
            if final_score >= 80:
                recommendation = "Ø®Ø±ÛŒØ¯ Ù‚ÙˆÛŒ"
            elif final_score >= 65:
                recommendation = "Ø®Ø±ÛŒØ¯"
            elif final_score >= 50:
                recommendation = "Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ"
            elif final_score >= 35:
                recommendation = "ÙØ±ÙˆØ´"
            else:
                recommendation = "ÙØ±ÙˆØ´ Ù‚ÙˆÛŒ"
            
            analysis_result = {
                'symbol': symbol,
                'timestamp': datetime.now().isoformat(),
                'scores': scores,
                'final_score': final_score,
                'recommendation': recommendation,
                'data_points_analyzed': len(raw_data)
            }
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡ ØªØ­Ù„ÛŒÙ„
            cursor.execute('''
                INSERT INTO analysis_results (timestamp, symbol, analysis_type, score, details)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                analysis_result['timestamp'],
                symbol,
                'comprehensive',
                final_score,
                json.dumps(analysis_result)
            ))
            
            # Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
            cursor.execute('''
                UPDATE raw_data SET processed = 1 
                WHERE symbol = ? AND DATE(timestamp) = DATE('now')
            ''', (symbol,))
            
            conn.commit()
            conn.close()
            
            # Ø´Ù…Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§
            self.analysis_count += 1
            
            # Ø«Ø¨Øª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
            if MONITORING_ENABLED:
                telemetry.record_metric("analysis.count", self.analysis_count, MetricType.COUNTER)
                telemetry.record_metric(f"analysis.{symbol}.final_score", final_score, MetricType.GAUGE)
                telemetry.record_metric("analysis.data_points", len(raw_data), MetricType.GAUGE)
                
                telemetry.record_event(
                    "analysis_completed",
                    {
                        "symbol": symbol,
                        "final_score": final_score,
                        "recommendation": recommendation,
                        "data_points": len(raw_data)
                    },
                    "info",
                    "DataAnalysis"
                )
            
            logger.info(f"âœ… ØªØ­Ù„ÛŒÙ„ {symbol}: Ø§Ù…ØªÛŒØ§Ø² {final_score:.1f} - {recommendation}")
            return analysis_result
            
        except Exception as e:
            if MONITORING_ENABLED:
                telemetry.record_error(
                    AlertLevel.ERROR,
                    f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ {symbol}: {str(e)}",
                    "DataAnalysis",
                    str(e)
                )
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ {symbol}: {e}")
            return {}
    
    def _calculate_price_score(self, raw_data: List) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø­Ø±Ú©Øª Ù‚ÛŒÙ…Øª"""
        prices = []
        for row in raw_data:
            data = json.loads(row[0])
            prices.append(data['price'])
        
        if len(prices) < 2:
            return 50
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±ÙˆÙ†Ø¯
        price_change = (prices[-1] - prices[0]) / prices[0] * 100
        
        # Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ÙˆÙ†Ø¯
        if price_change > 5:
            return 90
        elif price_change > 2:
            return 75
        elif price_change > -2:
            return 50
        elif price_change > -5:
            return 25
        else:
            return 10
    
    def _calculate_volume_score(self, raw_data: List) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""
        volumes = []
        for row in raw_data:
            data = json.loads(row[0])
            volumes.append(data['volume'])
        
        if not volumes:
            return 50
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù…
        avg_volume = np.mean(volumes)
        recent_volume = volumes[-1] if volumes else 0
        
        # Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø¬Ù…
        volume_ratio = recent_volume / avg_volume if avg_volume > 0 else 1
        
        if volume_ratio > 2:
            return 95
        elif volume_ratio > 1.5:
            return 80
        elif volume_ratio > 1:
            return 60
        elif volume_ratio > 0.7:
            return 40
        else:
            return 20
    
    def _calculate_news_sentiment(self, symbol: str) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø®Ø¨Ø±ÛŒ"""
        # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø®Ø¨Ø±ÛŒ
        # Ø¯Ø± Ø­Ø§Ù„Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² NewsAPI Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        return np.random.uniform(30, 80)
    
    def _calculate_technical_score(self, raw_data: List) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„"""
        if not raw_data:
            return 50
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ RSI, MACD, etc.
        # ÙØ¹Ù„Ø§Ù‹ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ
        return np.random.uniform(40, 85)
    
    def _get_ai_prediction_score(self, symbol: str, raw_data: List) -> float:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ù…ØªÛŒØ§Ø² Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ AI"""
        if not self.openai_client or not raw_data:
            return 50
        
        try:
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ AI
            recent_data = json.loads(raw_data[-1][0]) if raw_data else {}
            
            prompt = f"""
            Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø¨Ø±Ø§ÛŒ {symbol}ØŒ ÛŒÚ© Ø§Ù…ØªÛŒØ§Ø² Ø§Ø² 0 ØªØ§ 100 Ø¨Ø±Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø±Ø´Ø¯ Ù‚ÛŒÙ…Øª Ø¨Ø¯Ù‡:
            - Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ: ${recent_data.get('price', 0):.2f}
            - ØªØºÛŒÛŒØ± 24 Ø³Ø§Ø¹ØªÙ‡: {recent_data.get('change_24h', 0):.2f}%
            - Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {recent_data.get('volume', 0):.0f}
            
            ÙÙ‚Ø· Ø¹Ø¯Ø¯ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†.
            """
            
            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=10
            )
            
            score = float(response.choices[0].message.content.strip())
            return min(max(score, 0), 100)
        except:
            return 50
    
    @log_function_performance
    def transfer_to_mongodb(self):
        """Ø§Ù†ØªÙ‚Ø§Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ MongoDB Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ø´Ø¨"""
        logger.info("ğŸŒ™ Ø´Ø±ÙˆØ¹ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ MongoDB...")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ MongoDB
        mongodb_uri = os.environ.get('MONGODB_URI')
        if not mongodb_uri:
            logger.warning("âš ï¸ MongoDB URI ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ - Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØµÙ")
            self._save_to_queue()
            return
        
        try:
            from pymongo import MongoClient
            client = MongoClient(mongodb_uri, serverSelectionTimeoutMS=5000)
            db = client['ultra_plus_bot']
            
            # Ø§Ù†ØªÙ‚Ø§Ù„ Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡
            conn = sqlite3.connect(self.analysis_db)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT * FROM daily_summaries
                WHERE transferred_to_mongodb = 0
            ''')
            
            summaries = cursor.fetchall()
            
            if summaries:
                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª MongoDB
                documents = []
                for summary in summaries:
                    doc = {
                        '_id': f"{summary[1]}_{summary[2]}",  # date_symbol
                        'date': summary[1],
                        'symbol': summary[2],
                        'final_score': summary[3],
                        'recommendation': summary[4],
                        'analysis': json.loads(summary[5]),
                        'created_at': datetime.now()
                    }
                    documents.append(doc)
                
                # Ø¯Ø±Ø¬ Ø¯Ø± MongoDB
                collection = db['daily_analysis']
                result = collection.insert_many(documents, ordered=False)
                
                # Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ù…ÙˆÙÙ‚
                for summary in summaries:
                    cursor.execute('''
                        UPDATE daily_summaries SET transferred_to_mongodb = 1
                        WHERE id = ?
                    ''', (summary[0],))
                
                conn.commit()
                
                # Ø´Ù…Ø§Ø±Ø´ Ø§Ù†ØªÙ‚Ø§Ù„â€ŒÙ‡Ø§
                self.mongodb_transfers += 1
                
                # Ø«Ø¨Øª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
                if MONITORING_ENABLED:
                    telemetry.record_metric("mongodb.transfers", self.mongodb_transfers, MetricType.COUNTER)
                    telemetry.record_metric("mongodb.documents_transferred", len(result.inserted_ids), MetricType.GAUGE)
                    
                    telemetry.record_event(
                        "mongodb_transfer_completed",
                        {
                            "documents_count": len(result.inserted_ids),
                            "transfer_number": self.mongodb_transfers
                        },
                        "info",
                        "DataTransfer"
                    )
                
                logger.info(f"âœ… {len(result.inserted_ids)} Ø³Ù†Ø¯ Ø¨Ù‡ MongoDB Ù…Ù†ØªÙ‚Ù„ Ø´Ø¯")
            
            # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª
            self._cleanup_temp_data()
            
            client.close()
            conn.close()
            
        except Exception as e:
            if MONITORING_ENABLED:
                telemetry.record_error(
                    AlertLevel.ERROR,
                    f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ MongoDB: {str(e)}",
                    "DataTransfer",
                    str(e)
                )
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ MongoDB: {e}")
            self._save_to_queue()
    
    def _save_to_queue(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØµÙ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ø¹Ø¯ÛŒ"""
        conn = sqlite3.connect(self.analysis_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM daily_summaries
            WHERE transferred_to_mongodb = 0
        ''')
        
        summaries = cursor.fetchall()
        
        queue_data = []
        for summary in summaries:
            queue_data.append({
                'date': summary[1],
                'symbol': summary[2],
                'final_score': summary[3],
                'recommendation': summary[4],
                'analysis': json.loads(summary[5])
            })
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ JSON
        with open(self.mongodb_queue, 'w', encoding='utf-8') as f:
            json.dump(queue_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ {len(queue_data)} Ø±Ú©ÙˆØ±Ø¯ Ø¯Ø± ØµÙ MongoDB Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        conn.close()
    
    def _cleanup_temp_data(self):
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª"""
        conn = sqlite3.connect(self.temp_db)
        cursor = conn.cursor()
        
        # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø¨ÛŒØ´ Ø§Ø² 1 Ø±ÙˆØ²
        cursor.execute('''
            DELETE FROM raw_data 
            WHERE processed = 1 AND 
            datetime(timestamp) < datetime('now', '-1 day')
        ''')
        
        cursor.execute('''
            DELETE FROM analysis_results
            WHERE datetime(timestamp) < datetime('now', '-1 day')
        ''')
        
        conn.commit()
        conn.close()
        
        logger.info("ğŸ§¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯")
    
    def prepare_daily_summary(self):
        """Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®Ù„Ø§ØµÙ‡ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ MongoDB"""
        conn_temp = sqlite3.connect(self.temp_db)
        conn_analysis = sqlite3.connect(self.analysis_db)
        
        cursor_temp = conn_temp.cursor()
        cursor_analysis = conn_analysis.cursor()
        
        # Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ø±ÙˆØ²
        cursor_temp.execute('''
            SELECT DISTINCT symbol FROM analysis_results
            WHERE DATE(timestamp) = DATE('now')
        ''')
        
        symbols = cursor_temp.fetchall()
        
        for (symbol,) in symbols:
            # Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† ØªØ­Ù„ÛŒÙ„ Ù‡Ø± Ø³Ù…Ø¨Ù„
            cursor_temp.execute('''
                SELECT score, details FROM analysis_results
                WHERE symbol = ? AND DATE(timestamp) = DATE('now')
                ORDER BY timestamp DESC LIMIT 1
            ''', (symbol,))
            
            result = cursor_temp.fetchone()
            if result:
                score, details = result
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡ Ø±ÙˆØ²Ø§Ù†Ù‡
                cursor_analysis.execute('''
                    INSERT INTO daily_summaries (date, symbol, final_score, recommendation, analysis)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    datetime.now().date().isoformat(),
                    symbol,
                    score,
                    json.loads(details).get('recommendation', ''),
                    details
                ))
        
        conn_analysis.commit()
        conn_temp.close()
        conn_analysis.close()
        
        logger.info("ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯")
    
    def get_system_status(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…"""
        status = {
            'timestamp': datetime.now().isoformat(),
            'data_collection_count': self.data_collection_count,
            'analysis_count': self.analysis_count,
            'mongodb_transfers': self.mongodb_transfers,
            'recent_errors_count': len(self.last_collection_errors),
            'database_sizes': self.database_sizes,
            'monitoring_enabled': MONITORING_ENABLED,
            'openai_available': self.openai_client is not None,
            'last_mongodb_transfer': None  # TODO: track this
        }
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø±Ø® Ø®Ø·Ø§
        if self.data_collection_count > 0:
            status['error_rate'] = len(self.last_collection_errors) / self.data_collection_count
        else:
            status['error_rate'] = 0
        
        return status
    
    def start_collection_cycle(self):
        """Ø´Ø±ÙˆØ¹ Ú†Ø±Ø®Ù‡ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„"""
        logger.info("ğŸš€ Ø³ÛŒØ³ØªÙ… Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ ÙØ¹Ø§Ù„ Ø´Ø¯")
        logger.info(f"ğŸ”§ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡: {'ÙØ¹Ø§Ù„' if MONITORING_ENABLED else 'ØºÛŒØ±ÙØ¹Ø§Ù„'}")
        
        # Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ù‡Ø± 5 Ø¯Ù‚ÛŒÙ‚Ù‡)
        schedule.every(5).minutes.do(self._collect_all_markets)
        
        # Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ ØªØ­Ù„ÛŒÙ„ (Ù‡Ø± 30 Ø¯Ù‚ÛŒÙ‚Ù‡)
        schedule.every(30).minutes.do(self._analyze_all_symbols)
        
        # Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø®Ù„Ø§ØµÙ‡ Ø±ÙˆØ²Ø§Ù†Ù‡ (Ø³Ø§Ø¹Øª 23:00)
        schedule.every().day.at("23:00").do(self.prepare_daily_summary)
        
        # Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ MongoDB (Ø³Ø§Ø¹Øª 23:30)
        schedule.every().day.at("23:30").do(self.transfer_to_mongodb)
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        self._collect_all_markets()
        
        # Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ
        while True:
            schedule.run_pending()
            time_module.sleep(60)
    
    def _collect_all_markets(self):
        """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø² ØªÙ…Ø§Ù… Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§"""
        markets = [
            ('BTC/USDT', 'crypto'),
            ('ETH/USDT', 'crypto'),
            ('AAPL', 'stock'),
            ('GOOGL', 'stock'),
            ('EUR/USD', 'forex'),
            ('GOLD', 'commodity')
        ]
        
        for symbol, market in markets:
            try:
                self.collect_market_data(symbol, market)
            except Exception as e:
                logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ {symbol}: {e}")
    
    def _analyze_all_symbols(self):
        """ØªØ­Ù„ÛŒÙ„ ØªÙ…Ø§Ù… Ø³Ù…Ø¨Ù„â€ŒÙ‡Ø§"""
        conn = sqlite3.connect(self.temp_db)
        cursor = conn.cursor()
        
        cursor.execute('SELECT DISTINCT symbol FROM raw_data WHERE processed = 0')
        symbols = cursor.fetchall()
        
        for (symbol,) in symbols:
            try:
                self.analyze_and_score(symbol)
            except Exception as e:
                logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ {symbol}: {e}")
        
        conn.close()

# Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡
if __name__ == "__main__":
    collector = DailyDataCollectionSystem()
    
    # ØªØ³Øª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„
    try:
        collector.collect_market_data('BTC/USDT', 'crypto')
        result = collector.analyze_and_score('BTC/USDT')
        
        if result:
            print(f"\nğŸ“Š Ù†ØªÛŒØ¬Ù‡ ØªØ­Ù„ÛŒÙ„ {result['symbol']}:")
            print(f"Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ: {result['final_score']:.1f}")
            print(f"ØªÙˆØµÛŒÙ‡: {result['recommendation']}")
        
        # Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…
        status = collector.get_system_status()
        print(f"\nğŸ”§ ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…:")
        print(f"   Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {status['data_collection_count']}")
        print(f"   ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§: {status['analysis_count']}")
        print(f"   Ù†Ø±Ø® Ø®Ø·Ø§: {status['error_rate']:.1%}")
        print(f"   Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡: {status['monitoring_enabled']}")
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª: {e}")
    
    # Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„:
    # collector.start_collection_cycle()
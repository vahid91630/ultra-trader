#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø³ÛŒØ³ØªÙ… Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ø§ Ø§Ù†ØªÙ‚Ø§Ù„ Ø´Ø¨Ø§Ù†Ù‡ Ø¨Ù‡ MongoDB
"""

import os
import json
import sqlite3
from datetime import datetime, time, timedelta
import schedule
import threading
import logging
import time as time_module
from typing import Dict, List, Any
import numpy as np
from openai import OpenAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DailyDataCollectionSystem:
    def __init__(self):
        self.temp_db = 'temp_daily_data.db'
        self.analysis_db = 'daily_analysis.db'
        self.mongodb_queue = 'mongodb_queue.json'
        
        # Ø³Ø§Ø¹Øª Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ MongoDB (23:30)
        self.transfer_time = time(23, 30)
        
        # Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ùˆ ÙˆØ²Ù†â€ŒÙ‡Ø§
        self.scoring_weights = {
            'price_movement': 0.25,
            'volume': 0.20,
            'news_sentiment': 0.20,
            'technical_indicators': 0.15,
            'ai_prediction': 0.20
        }
        
        # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        self._initialize_databases()
        self.openai_client = OpenAI() if os.environ.get('OPENAI_API_KEY') else None
    
    def _initialize_databases(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯Ø§ÙˆÙ„ Ù…ÙˆÙ‚Øª Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡"""
        # Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆÙ‚Øª Ø±ÙˆØ²Ø§Ù†Ù‡
        conn = sqlite3.connect(self.temp_db)
        cursor = conn.cursor()
        
        # Ø¬Ø¯ÙˆÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS raw_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                data_type TEXT,
                market TEXT,
                symbol TEXT,
                data JSON,
                processed BOOLEAN DEFAULT 0
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analysis_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                symbol TEXT,
                analysis_type TEXT,
                score REAL,
                details JSON
            )
        ''')
        
        conn.commit()
        conn.close()
        
        # Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        conn = sqlite3.connect(self.analysis_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS daily_summaries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT,
                symbol TEXT,
                final_score REAL,
                recommendation TEXT,
                analysis JSON,
                transferred_to_mongodb BOOLEAN DEFAULT 0
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def collect_market_data(self, symbol: str, market: str) -> Dict:
        """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±"""
        data = {
            'timestamp': datetime.now().isoformat(),
            'symbol': symbol,
            'market': market,
            'price': np.random.uniform(100, 50000),  # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù‚ÛŒÙ…Øª
            'volume': np.random.uniform(1000, 100000),
            'change_24h': np.random.uniform(-10, 10),
            'high_24h': np.random.uniform(100, 51000),
            'low_24h': np.random.uniform(99, 49000)
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆÙ‚Øª
        conn = sqlite3.connect(self.temp_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO raw_data (timestamp, data_type, market, symbol, data)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            data['timestamp'],
            'market_data',
            market,
            symbol,
            json.dumps(data)
        ))
        
        conn.commit()
        conn.close()
        
        logger.info(f"ğŸ“Š Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯: {symbol} - Ù‚ÛŒÙ…Øª: ${data['price']:.2f}")
        return data
    
    def analyze_and_score(self, symbol: str) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ùˆ Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        conn = sqlite3.connect(self.temp_db)
        cursor = conn.cursor()
        
        # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ù…Ø±ÙˆØ²
        cursor.execute('''
            SELECT data FROM raw_data 
            WHERE symbol = ? AND DATE(timestamp) = DATE('now')
            AND processed = 0
        ''', (symbol,))
        
        raw_data = cursor.fetchall()
        
        if not raw_data:
            return {}
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª
        scores = {
            'price_movement': self._calculate_price_score(raw_data),
            'volume': self._calculate_volume_score(raw_data),
            'news_sentiment': self._calculate_news_sentiment(symbol),
            'technical_indicators': self._calculate_technical_score(raw_data),
            'ai_prediction': self._get_ai_prediction_score(symbol, raw_data)
        }
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ
        final_score = sum(
            scores[key] * self.scoring_weights[key] 
            for key in scores
        )
        
        # ØªØ¹ÛŒÛŒÙ† ØªÙˆØµÛŒÙ‡
        if final_score >= 80:
            recommendation = "Ø®Ø±ÛŒØ¯ Ù‚ÙˆÛŒ"
        elif final_score >= 65:
            recommendation = "Ø®Ø±ÛŒØ¯"
        elif final_score >= 50:
            recommendation = "Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ"
        elif final_score >= 35:
            recommendation = "ÙØ±ÙˆØ´"
        else:
            recommendation = "ÙØ±ÙˆØ´ Ù‚ÙˆÛŒ"
        
        analysis_result = {
            'symbol': symbol,
            'timestamp': datetime.now().isoformat(),
            'scores': scores,
            'final_score': final_score,
            'recommendation': recommendation,
            'data_points_analyzed': len(raw_data)
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡ ØªØ­Ù„ÛŒÙ„
        cursor.execute('''
            INSERT INTO analysis_results (timestamp, symbol, analysis_type, score, details)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            analysis_result['timestamp'],
            symbol,
            'comprehensive',
            final_score,
            json.dumps(analysis_result)
        ))
        
        # Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
        cursor.execute('''
            UPDATE raw_data SET processed = 1 
            WHERE symbol = ? AND DATE(timestamp) = DATE('now')
        ''', (symbol,))
        
        conn.commit()
        conn.close()
        
        logger.info(f"âœ… ØªØ­Ù„ÛŒÙ„ {symbol}: Ø§Ù…ØªÛŒØ§Ø² {final_score:.1f} - {recommendation}")
        return analysis_result
    
    def _calculate_price_score(self, raw_data: List) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø­Ø±Ú©Øª Ù‚ÛŒÙ…Øª"""
        prices = []
        for row in raw_data:
            data = json.loads(row[0])
            prices.append(data['price'])
        
        if len(prices) < 2:
            return 50
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±ÙˆÙ†Ø¯
        price_change = (prices[-1] - prices[0]) / prices[0] * 100
        
        # Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ÙˆÙ†Ø¯
        if price_change > 5:
            return 90
        elif price_change > 2:
            return 75
        elif price_change > -2:
            return 50
        elif price_change > -5:
            return 25
        else:
            return 10
    
    def _calculate_volume_score(self, raw_data: List) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""
        volumes = []
        for row in raw_data:
            data = json.loads(row[0])
            volumes.append(data['volume'])
        
        if not volumes:
            return 50
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù…
        avg_volume = np.mean(volumes)
        recent_volume = volumes[-1] if volumes else 0
        
        # Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø¬Ù…
        volume_ratio = recent_volume / avg_volume if avg_volume > 0 else 1
        
        if volume_ratio > 2:
            return 95
        elif volume_ratio > 1.5:
            return 80
        elif volume_ratio > 1:
            return 60
        elif volume_ratio > 0.7:
            return 40
        else:
            return 20
    
    def _calculate_news_sentiment(self, symbol: str) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø®Ø¨Ø±ÛŒ"""
        # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø®Ø¨Ø±ÛŒ
        # Ø¯Ø± Ø­Ø§Ù„Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² NewsAPI Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        return np.random.uniform(30, 80)
    
    def _calculate_technical_score(self, raw_data: List) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„"""
        if not raw_data:
            return 50
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ RSI, MACD, etc.
        # ÙØ¹Ù„Ø§Ù‹ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ
        return np.random.uniform(40, 85)
    
    def _get_ai_prediction_score(self, symbol: str, raw_data: List) -> float:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ù…ØªÛŒØ§Ø² Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ AI"""
        if not self.openai_client or not raw_data:
            return 50
        
        try:
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ AI
            recent_data = json.loads(raw_data[-1][0]) if raw_data else {}
            
            prompt = f"""
            Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø¨Ø±Ø§ÛŒ {symbol}ØŒ ÛŒÚ© Ø§Ù…ØªÛŒØ§Ø² Ø§Ø² 0 ØªØ§ 100 Ø¨Ø±Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø±Ø´Ø¯ Ù‚ÛŒÙ…Øª Ø¨Ø¯Ù‡:
            - Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ: ${recent_data.get('price', 0):.2f}
            - ØªØºÛŒÛŒØ± 24 Ø³Ø§Ø¹ØªÙ‡: {recent_data.get('change_24h', 0):.2f}%
            - Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {recent_data.get('volume', 0):.0f}
            
            ÙÙ‚Ø· Ø¹Ø¯Ø¯ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†.
            """
            
            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=10
            )
            
            score = float(response.choices[0].message.content.strip())
            return min(max(score, 0), 100)
        except:
            return 50
    
    def prepare_daily_summary(self):
        """Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®Ù„Ø§ØµÙ‡ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ MongoDB"""
        conn_temp = sqlite3.connect(self.temp_db)
        conn_analysis = sqlite3.connect(self.analysis_db)
        
        cursor_temp = conn_temp.cursor()
        cursor_analysis = conn_analysis.cursor()
        
        # Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ø±ÙˆØ²
        cursor_temp.execute('''
            SELECT DISTINCT symbol FROM analysis_results
            WHERE DATE(timestamp) = DATE('now')
        ''')
        
        symbols = cursor_temp.fetchall()
        
        for (symbol,) in symbols:
            # Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† ØªØ­Ù„ÛŒÙ„ Ù‡Ø± Ø³Ù…Ø¨Ù„
            cursor_temp.execute('''
                SELECT score, details FROM analysis_results
                WHERE symbol = ? AND DATE(timestamp) = DATE('now')
                ORDER BY timestamp DESC LIMIT 1
            ''', (symbol,))
            
            result = cursor_temp.fetchone()
            if result:
                score, details = result
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡ Ø±ÙˆØ²Ø§Ù†Ù‡
                cursor_analysis.execute('''
                    INSERT INTO daily_summaries (date, symbol, final_score, recommendation, analysis)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    datetime.now().date().isoformat(),
                    symbol,
                    score,
                    json.loads(details).get('recommendation', ''),
                    details
                ))
        
        conn_analysis.commit()
        conn_temp.close()
        conn_analysis.close()
        
        logger.info("ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯")
    
    def transfer_to_mongodb(self):
        """Ø§Ù†ØªÙ‚Ø§Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ MongoDB Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ø´Ø¨ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ù‡ØªØ± Ø®Ø·Ø§"""
        logger.info("ğŸŒ™ Ø´Ø±ÙˆØ¹ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ MongoDB...")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ MongoDB
        mongodb_uri = os.environ.get('MONGODB_URI') or os.environ.get('MONGO_URI')
        if not mongodb_uri:
            logger.error("âŒ Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù… Ø§Ø² Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ MONGODB_URI ÛŒØ§ MONGO_URI ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡")
            logger.info("ğŸ’¡ Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ:")
            logger.info("   - MONGODB_URI=mongodb+srv://username:password@cluster.mongodb.net/database")
            logger.info("   - ÛŒØ§ MONGO_URI=mongodb://localhost:27017/ultra_trader")
            logger.info("   - ÛŒØ§ Ø¯Ø± ÙØ§ÛŒÙ„ .env ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯")
            self._save_to_queue("Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ MongoDB ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡")
            return
        
        try:
            from pymongo import MongoClient
            from pymongo.errors import ServerSelectionTimeoutError, ConfigurationError
            
            logger.info("ğŸ”Œ Ø¯Ø± Ø­Ø§Ù„ Ø§ØªØµØ§Ù„ Ø¨Ù‡ MongoDB...")
            client = MongoClient(mongodb_uri, serverSelectionTimeoutMS=10000)
            
            # ØªØ³Øª Ø§ØªØµØ§Ù„
            client.admin.command('ping')
            logger.info("âœ… Ø§ØªØµØ§Ù„ MongoDB Ø¨Ø±Ù‚Ø±Ø§Ø± Ø´Ø¯")
            
            db = client['ultra_plus_bot']
            
            # Ø§Ù†ØªÙ‚Ø§Ù„ Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡
            conn = sqlite3.connect(self.analysis_db)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT * FROM daily_summaries
                WHERE transferred_to_mongodb = 0
            ''')
            
            summaries = cursor.fetchall()
            
            if summaries:
                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª MongoDB
                documents = []
                for summary in summaries:
                    doc = {
                        '_id': f"{summary[1]}_{summary[2]}",  # date_symbol
                        'date': summary[1],
                        'symbol': summary[2],
                        'final_score': summary[3],
                        'recommendation': summary[4],
                        'analysis': json.loads(summary[5]),
                        'created_at': datetime.now(),
                        'transfer_timestamp': datetime.now().isoformat()
                    }
                    documents.append(doc)
                
                # Ø¯Ø±Ø¬ Ø¯Ø± MongoDB
                collection = db['daily_analysis']
                try:
                    result = collection.insert_many(documents, ordered=False)
                    logger.info(f"âœ… {len(result.inserted_ids)} Ø³Ù†Ø¯ Ø¨Ù‡ MongoDB Ù…Ù†ØªÙ‚Ù„ Ø´Ø¯")
                    
                    # Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ù…ÙˆÙÙ‚
                    for summary in summaries:
                        cursor.execute('''
                            UPDATE daily_summaries SET transferred_to_mongodb = 1
                            WHERE id = ?
                        ''', (summary[0],))
                    
                    conn.commit()
                    
                except Exception as insert_error:
                    logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {insert_error}")
                    self._save_to_queue(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±Ø¬: {str(insert_error)}")
            else:
                logger.info("â„¹ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª")
            
            # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª
            self._cleanup_temp_data()
            
            client.close()
            conn.close()
            
        except ImportError:
            logger.error("âŒ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ pymongo Ù†ØµØ¨ Ù†ÛŒØ³Øª")
            logger.info("ğŸ’¡ Ø¨Ø±Ø§ÛŒ Ù†ØµØ¨: pip install pymongo")
            self._save_to_queue("pymongo Ù†ØµØ¨ Ù†ÛŒØ³Øª")
            
        except ServerSelectionTimeoutError as e:
            logger.error(f"ğŸ• Timeout Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ MongoDB: {e}")
            logger.info("ğŸ’¡ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯:")
            logger.info("   - Ø§ØªØµØ§Ù„ Ø§ÛŒÙ†ØªØ±Ù†Øª")
            logger.info("   - Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø³Ø±ÙˆØ± MongoDB")
            logger.info("   - ØµØ­Øª Ø¢Ø¯Ø±Ø³ Ø³Ø±ÙˆØ±")
            self._save_to_queue(f"Timeout Ø§ØªØµØ§Ù„: {str(e)}")
            
        except ConfigurationError as e:
            logger.error(f"âš™ï¸ Ø®Ø·Ø§ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ MongoDB: {e}")
            logger.info("ğŸ’¡ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯:")
            logger.info("   - ÙØ±Ù…Øª URI ØµØ­ÛŒØ­ Ø¨Ø§Ø´Ø¯")
            logger.info("   - Ù†Ø§Ù… Ú©Ø§Ø±Ø¨Ø±ÛŒ Ùˆ Ø±Ù…Ø² Ø¹Ø¨ÙˆØ±")
            self._save_to_queue(f"Ø®Ø·Ø§ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ: {str(e)}")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ MongoDB: {e}")
            logger.info(f"ğŸ”§ Ø¬Ø²Ø¦ÛŒØ§Øª: Ù†ÙˆØ¹ Ø®Ø·Ø§ {type(e).__name__}")
            self._save_to_queue(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {str(e)}")
    
    def _save_to_queue(self, error_reason="Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ"):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØµÙ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±"""
        logger.info(f"ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ØµÙ (Ø¯Ù„ÛŒÙ„: {error_reason})")
        
        conn = sqlite3.connect(self.analysis_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM daily_summaries
            WHERE transferred_to_mongodb = 0
        ''')
        
        summaries = cursor.fetchall()
        
        queue_data = {
            'error_reason': error_reason,
            'timestamp': datetime.now().isoformat(),
            'retry_count': 0,
            'data': []
        }
        
        for summary in summaries:
            queue_data['data'].append({
                'date': summary[1],
                'symbol': summary[2],
                'final_score': summary[3],
                'recommendation': summary[4],
                'analysis': json.loads(summary[5])
            })
        
        # Ø®ÙˆØ§Ù†Ø¯Ù† ØµÙ Ù‚Ø¯ÛŒÙ…ÛŒ Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø¢Ù†
        try:
            with open(self.mongodb_queue, 'r', encoding='utf-8') as f:
                existing_queue = json.load(f)
                if isinstance(existing_queue, list):
                    # ÙØ±Ù…Øª Ù‚Ø¯ÛŒÙ…ÛŒ - ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ø¬Ø¯ÛŒØ¯
                    queue_data['data'].extend(existing_queue)
                elif isinstance(existing_queue, dict):
                    # ÙØ±Ù…Øª Ø¬Ø¯ÛŒØ¯ - Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
                    queue_data['data'].extend(existing_queue.get('data', []))
                    queue_data['retry_count'] = existing_queue.get('retry_count', 0) + 1
        except (FileNotFoundError, json.JSONDecodeError):
            # ÙØ§ÛŒÙ„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ ÛŒØ§ ÙØ±Ù…Øª Ø§Ø´ØªØ¨Ø§Ù‡ - Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            pass
        
        # Ø°Ø®ÛŒØ±Ù‡ ØµÙ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯Ù‡
        with open(self.mongodb_queue, 'w', encoding='utf-8') as f:
            json.dump(queue_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ {len(queue_data['data'])} Ø±Ú©ÙˆØ±Ø¯ Ø¯Ø± ØµÙ MongoDB Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ (ØªÙ„Ø§Ø´ #{queue_data['retry_count']})")
        conn.close()
    
    def _cleanup_temp_data(self):
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª"""
        conn = sqlite3.connect(self.temp_db)
        cursor = conn.cursor()
        
        # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø¨ÛŒØ´ Ø§Ø² 1 Ø±ÙˆØ²
        cursor.execute('''
            DELETE FROM raw_data 
            WHERE processed = 1 AND 
            datetime(timestamp) < datetime('now', '-1 day')
        ''')
        
        cursor.execute('''
            DELETE FROM analysis_results
            WHERE datetime(timestamp) < datetime('now', '-1 day')
        ''')
        
        conn.commit()
        conn.close()
        
        logger.info("ğŸ§¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯")
    
    def start_collection_cycle(self):
        """Ø´Ø±ÙˆØ¹ Ú†Ø±Ø®Ù‡ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„"""
        logger.info("ğŸš€ Ø³ÛŒØ³ØªÙ… Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ ÙØ¹Ø§Ù„ Ø´Ø¯")
        
        # Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ù‡Ø± 5 Ø¯Ù‚ÛŒÙ‚Ù‡)
        schedule.every(5).minutes.do(self._collect_all_markets)
        
        # Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ ØªØ­Ù„ÛŒÙ„ (Ù‡Ø± 30 Ø¯Ù‚ÛŒÙ‚Ù‡)
        schedule.every(30).minutes.do(self._analyze_all_symbols)
        
        # Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø®Ù„Ø§ØµÙ‡ Ø±ÙˆØ²Ø§Ù†Ù‡ (Ø³Ø§Ø¹Øª 23:00)
        schedule.every().day.at("23:00").do(self.prepare_daily_summary)
        
        # Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ MongoDB (Ø³Ø§Ø¹Øª 23:30)
        schedule.every().day.at("23:30").do(self.transfer_to_mongodb)
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        self._collect_all_markets()
        
        # Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ
        while True:
            schedule.run_pending()
            time_module.sleep(60)
    
    def _collect_all_markets(self):
        """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø² ØªÙ…Ø§Ù… Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§"""
        markets = [
            ('BTC/USDT', 'crypto'),
            ('ETH/USDT', 'crypto'),
            ('AAPL', 'stock'),
            ('GOOGL', 'stock'),
            ('EUR/USD', 'forex'),
            ('GOLD', 'commodity')
        ]
        
        for symbol, market in markets:
            self.collect_market_data(symbol, market)
    
    def _analyze_all_symbols(self):
        """ØªØ­Ù„ÛŒÙ„ ØªÙ…Ø§Ù… Ø³Ù…Ø¨Ù„â€ŒÙ‡Ø§"""
        conn = sqlite3.connect(self.temp_db)
        cursor = conn.cursor()
        
        cursor.execute('SELECT DISTINCT symbol FROM raw_data WHERE processed = 0')
        symbols = cursor.fetchall()
        
        for (symbol,) in symbols:
            self.analyze_and_score(symbol)
        
        conn.close()

# Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡
if __name__ == "__main__":
    collector = DailyDataCollectionSystem()
    
    # ØªØ³Øª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„
    collector.collect_market_data('BTC/USDT', 'crypto')
    result = collector.analyze_and_score('BTC/USDT')
    
    if result:
        print(f"\nğŸ“Š Ù†ØªÛŒØ¬Ù‡ ØªØ­Ù„ÛŒÙ„ {result['symbol']}:")
        print(f"Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ: {result['final_score']:.1f}")
        print(f"ØªÙˆØµÛŒÙ‡: {result['recommendation']}")
        
    # Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„:
    # collector.start_collection_cycle()